{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing running databricks locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting spark session\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import row_number, udf, col, sys, datediff, to_date, mean, stddev\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name  Age\n",
      "0   Scott   50\n",
      "1    Jeff   45\n",
      "2  Thomas   54\n",
      "3     Ann   34\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Scott| 50|\n",
      "|  Jeff| 45|\n",
      "|Thomas| 54|\n",
      "|   Ann| 34|\n",
      "+------+---+\n",
      "\n",
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n",
      "+----------+---+\n",
      "|First Name|Age|\n",
      "+----------+---+\n",
      "|     Scott| 50|\n",
      "|      Jeff| 45|\n",
      "|    Thomas| 54|\n",
      "|       Ann| 34|\n",
      "+----------+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Scott| 50|\n",
      "|  Jeff| 45|\n",
      "|Thomas| 54|\n",
      "|   Ann| 34|\n",
      "+------+---+\n",
      "\n",
      "<bound method PandasConversionMixin.toPandas of DataFrame[Name: string, Age: bigint]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd   \n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    " \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sparkDF=spark.createDataFrame(pandasDF) \n",
    "sparkDF.printSchema()\n",
    "sparkDF.show()\n",
    "\n",
    "#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \n",
    "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n",
    "\n",
    "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "# Enable Apache Arrow to convert Pandas to PySpark DataFrame\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "sparkDF2=spark.createDataFrame(pandasDF) \n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n",
    "#Convert PySpark DataFrame to Pandas\n",
    "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
    "print(pandasDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_code( x):\n",
    "    \"\"\"\n",
    "    Turns states to state code\n",
    "    \n",
    "    :param x : The string in question\n",
    "    :returns: prediction of the intended state code\n",
    "    \"\"\"\n",
    "    states = {'AK': 'Alaska','AL': 'Alabama','AR': 'Arkansas','AS': 'American Samoa','AZ': 'Arizona',\n",
    "                'CA': 'California', 'CO': 'Colorado','CT': 'Connecticut','DC': 'District of Columbia',\n",
    "                'DE': 'Delaware','FL': 'Florida','GA': 'Georgia','GU': 'Guam','HI': 'Hawaii','IA': 'Iowa',\n",
    "                'ID': 'Idaho','IL': 'Illinois','IN': 'Indiana','KS': 'Kansas','KY': 'Kentucky','LA': 'Louisiana',\n",
    "                'MA': 'Massachusetts','MD': 'Maryland','ME': 'Maine','MI': 'Michigan','MN': 'Minnesota',\n",
    "                'MO': 'Missouri','MP': 'Northern Mariana Islands','MS': 'Mississippi','MT': 'Montana',\n",
    "                'NC': 'North Carolina','ND': 'North Dakota','NE': 'Nebraska','NH': 'New ','NJ': 'New Jersey',\n",
    "                'NM': 'New Mexico','NV': 'Nevada','NY': 'New York','OH': 'Ohio','OK': 'Oklahoma','OR': 'Oregon',\n",
    "                'PA': 'Pennsylvania','PR': 'Puerto Rico','RI': 'Rhode Island','SC': 'South Carolina',\n",
    "                'SD': 'South Dakota','TN': 'Tennessee','TX': 'Texas','UT': 'Utah','VA': 'Virginia',\n",
    "                'VI': 'Virgin Islands','VT': 'Vermont','WA': 'Washington','WI': 'Wisconsin','WV': 'West Virginia',\n",
    "                'WY': 'Wyoming'}\n",
    "    \n",
    "    if len(x) == 2: # Try another way for 2-letter codes\n",
    "        for a,n in states.items():\n",
    "            if len(n.split()) == 2:\n",
    "                if \"\".join([c[0] for c in n.split()]).lower() == x.lower():\n",
    "                    return a.upper()\n",
    "    new_rx = re.compile(r\"\\w*\".join([ch for ch in x]), re.I)\n",
    "    for a,n in states.items():\n",
    "        if new_rx.match(n):\n",
    "            return a.upper()\n",
    "\n",
    "state_udf = udf(state_to_code, StringType()) #\n",
    "\n",
    "def remove_outliers(df,columns,n_std):\n",
    "    \"\"\"\n",
    "    Function to remove any value past a number of standard deviations from the mean of values in a dateframe\n",
    "    \n",
    "    :param df: The dateframe you want to remove the outliers from\n",
    "    :param columns: The name of the columns\n",
    "    :returns: Returns dataframe with outliers removed\n",
    "    \"\"\"\n",
    "    \n",
    "    for column in columns:\n",
    "        avg = df.agg({column: 'mean'})\n",
    "        av = avg.first()[f'avg({column})']\n",
    "        std = df.agg({column: 'stddev'})\n",
    "        sd = std.first()[f'stddev({column})']\n",
    "\n",
    "        df = df[(df[column] <= av + (n_std * sd))]\n",
    "        df = df[(df[column] >= av - (n_std * sd))]\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- start_balance: double (nullable = true)\n",
      " |-- creation_date: string (nullable = true)\n",
      "\n",
      "+----------+----------+-----------+--------+----------------+-------+----------+\n",
      "|      date|account_id|customer_id|  amount|transaction_date|deposit|withdrawal|\n",
      "+----------+----------+-----------+--------+----------------+-------+----------+\n",
      "|2007-01-31|  24137947|         91| 3034.26|      2007-01-31|3034.26|       0.0|\n",
      "|2007-01-31|  24137947|         91|-5295.18|      2007-01-16|    0.0|  -5295.18|\n",
      "|2007-02-28|  24137947|         91|     0.0|      2007-02-28|    0.0|       0.0|\n",
      "|2007-03-31|  24137947|         91|    -0.0|      2007-03-30|    0.0|      -0.0|\n",
      "|2007-03-31|  24137947|         91|    -0.0|      2007-03-11|    0.0|      -0.0|\n",
      "|2007-03-31|  24137947|         91|    -0.0|      2007-03-20|    0.0|      -0.0|\n",
      "|2007-01-31|  24137948|         92|     0.0|      2007-01-31|    0.0|       0.0|\n",
      "|2007-02-28|  24137948|         92|  1164.9|      2007-02-28| 1164.9|       0.0|\n",
      "|2007-03-31|  24137948|         92| 1257.38|      2007-03-31|1257.38|       0.0|\n",
      "|2007-04-30|  24137948|         92| 1338.12|      2007-04-30|1338.12|       0.0|\n",
      "|2007-05-31|  24137948|         92| 1393.85|      2007-05-31|1393.85|       0.0|\n",
      "|2007-05-31|  24137948|         92|-1556.51|      2007-05-14|    0.0|  -1556.51|\n",
      "|2007-05-31|  24137948|         92| -548.83|      2007-05-24|    0.0|   -548.83|\n",
      "|2007-06-30|  24137948|         92|     0.0|      2007-06-30|    0.0|       0.0|\n",
      "|2007-07-31|  24137948|         92| 1494.26|      2007-07-31|1494.26|       0.0|\n",
      "|2007-08-31|  24137948|         92| 1345.73|      2007-08-31|1345.73|       0.0|\n",
      "|2007-08-31|  24137948|         92| -319.79|      2007-08-06|    0.0|   -319.79|\n",
      "|2007-08-31|  24137948|         92| -104.97|      2007-08-23|    0.0|   -104.97|\n",
      "|2007-08-31|  24137948|         92| -276.08|      2007-08-05|    0.0|   -276.08|\n",
      "|2007-08-31|  24137948|         92|  -80.87|      2007-08-15|    0.0|    -80.87|\n",
      "+----------+----------+-----------+--------+----------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, account_id: bigint, customer_id: bigint, amount: double, transaction_date: string, deposit: double, withdrawal: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "cust_pd = pd.read_csv(\"data/customers_data.csv\")\n",
    "tran_pd = pd.read_csv(\"data/transactions_data.csv\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "cust_df = spark.createDataFrame(cust_pd) \n",
    "tran_df = spark.createDataFrame(tran_pd[:100000]) \n",
    "cust_df.printSchema()\n",
    "tran_df.show()\n",
    "\n",
    "\n",
    "# # remove outliers and null\n",
    "# cust_df = cust_df.dropna()\n",
    "# tran_df =tran_df.dropna()\n",
    "# cust_df = remove_outliers(cust_df, ['start_balance'], 4)         \n",
    "# tran_df = remove_outliers(tran_df, ['deposit', 'amount', 'withdrawal'], 4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+----------+-------+------+-----+-------------+------------------+--------------+----+-------+\n",
      "|account_id|      date|             amount|withdrawal|deposit|volume|state|start_balance|           balance|account_length| age|isChurn|\n",
      "+----------+----------+-------------------+----------+-------+------+-----+-------------+------------------+--------------+----+-------+\n",
      "|  24137947|2007-01-31|           -2260.92|  -5295.18|3034.26|     2|   CA|     10180.56| 7919.639999999999|             0|4962|      0|\n",
      "|  24137947|2007-02-28|                0.0|       0.0|    0.0|     1|   CA|     10180.56| 7919.639999999999|            28|4990|      0|\n",
      "|  24137947|2007-03-31|                0.0|       0.0|    0.0|     3|   CA|     10180.56| 7919.639999999999|            59|5021|      1|\n",
      "|  24137948|2007-01-31|                0.0|       0.0|    0.0|     1|   NY|      4757.68|           4757.68|             0|7727|      0|\n",
      "|  24137948|2007-02-28|             1164.9|       0.0| 1164.9|     1|   NY|      4757.68|           5922.58|            28|7755|      0|\n",
      "|  24137948|2007-03-31|            1257.38|       0.0|1257.38|     1|   NY|      4757.68| 7179.960000000001|            59|7786|      0|\n",
      "|  24137948|2007-04-30|            1338.12|       0.0|1338.12|     1|   NY|      4757.68|           8518.08|            89|7816|      0|\n",
      "|  24137948|2007-05-31| -711.4900000000001|  -2105.34|1393.85|     3|   NY|      4757.68|           7806.59|           120|7847|      0|\n",
      "|  24137948|2007-06-30|                0.0|       0.0|    0.0|     1|   NY|      4757.68|           7806.59|           150|7877|      0|\n",
      "|  24137948|2007-07-31|            1494.26|       0.0|1494.26|     1|   NY|      4757.68|           9300.85|           181|7908|      0|\n",
      "|  24137948|2007-08-31| -791.1099999999999|  -2136.84|1345.73|    10|   NY|      4757.68| 8509.740000000002|           212|7939|      0|\n",
      "|  24137948|2007-09-30|            1277.93|       0.0|1277.93|     1|   NY|      4757.68| 9787.670000000002|           242|7969|      0|\n",
      "|  24137948|2007-10-31| -531.4099999999999|  -1796.54|1265.13|     2|   NY|      4757.68| 9256.260000000002|           273|8000|      0|\n",
      "|  24137948|2007-11-30|-254.27999999999997|  -1590.43|1336.15|     5|   NY|      4757.68| 9001.980000000001|           303|8030|      0|\n",
      "|  24137948|2007-12-31|            1418.64|       0.0|1418.64|     1|   NY|      4757.68|10420.620000000003|           334|8061|      0|\n",
      "|  24137948|2008-01-31|                0.0|       0.0|    0.0|     1|   NY|      4757.68|10420.620000000003|           365|8092|      0|\n",
      "|  24137948|2008-02-29|                0.0|       0.0|    0.0|     1|   NY|      4757.68|10420.620000000003|           394|8121|      0|\n",
      "|  24137948|2008-03-31|                0.0|       0.0|    0.0|     1|   NY|      4757.68|10420.620000000003|           425|8152|      1|\n",
      "|  24137949|2007-01-31|            1809.75|       0.0|1809.75|     1|   MN|      6796.72| 8606.470000000001|             0|7013|      0|\n",
      "|  24137949|2007-02-28|            1962.01|       0.0|1962.01|     1|   MN|      6796.72|          10568.48|            28|7041|      0|\n",
      "+----------+----------+-------------------+----------+-------+------+-----+-------------+------------------+--------------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = tran_df\n",
    "\n",
    "# Add a column for volume\n",
    "w = Window().orderBy(['account_id', \"date\"])\n",
    "df = df.withColumn(\"volume\", row_number().over(w))\n",
    "# Aggragate to monthly data\n",
    "df = df.groupby([\"account_id\", \"date\"]).agg({\"account_id\": \"mean\",\n",
    "                                            \"customer_id\": \"mean\",\n",
    "                                            \"amount\": \"sum\",\n",
    "                                            \"deposit\":\"sum\",\n",
    "                                            \"withdrawal\":\"sum\",\n",
    "                                            \"volume\": \"count\"\n",
    "                                            })\n",
    "\n",
    "df = df.withColumnRenamed(\"avg(customer_id)\", \"customer_id\") \\\n",
    "       .withColumnRenamed(\"sum(amount)\",\"amount\")\\\n",
    "       .withColumnRenamed(\"sum(withdrawal)\",\"withdrawal\")\\\n",
    "       .withColumnRenamed(\"sum(deposit)\",\"deposit\")\\\n",
    "       .withColumnRenamed(\"count(volume)\",\"volume\") \\\n",
    "       .drop(\"avg(account_id)\") \\\n",
    "       .withColumn(\"customer_id\", col(\"customer_id\").cast(\"double\"))\n",
    "\n",
    "# Join customer infromation to transactions\n",
    "df = df.join(cust_df,\"customer_id\")  \n",
    "\n",
    "df= df.withColumn(\"state\", state_udf(\"state\")) #\n",
    "df = df.withColumn('balance', F.sum(df[\"amount\"]).over(Window.partitionBy('account_id').orderBy(\"date\").rowsBetween(-sys.maxsize, 0)))\n",
    "df = df.withColumn(\"balance\", col(\"balance\")+col(\"start_balance\"))\n",
    "df= df.withColumn('account_length',datediff(col(\"date\"),col(\"creation_date\")))\n",
    "df= df.withColumn('age',datediff(col(\"date\"),col(\"dob\")))\n",
    "# Calculate churn\n",
    "w2 = Window.partitionBy(\"account_id\")\n",
    "df = df.withColumn(\"last_date\", F.max(\"date\").over(w2))\n",
    "df = df.withColumn('isChurn', F.when((F.col(\"last_date\") == col('date')),1).otherwise(0))\n",
    "\n",
    "df = df.drop(\"customer_id\",\"dob\", \"creation_date\",\"last_date\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:114: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+---------------+-------+----------+---------+------------+\n",
      "|      date|interest_rate|real_income|p_change_income|UMCSENT|unemp_rate|    GDPC1|p_change_gdp|\n",
      "+----------+-------------+-----------+---------------+-------+----------+---------+------------+\n",
      "|2007-01-31|         6.25|    11439.6|         0.3148|   91.3|       4.5|15767.146| 0.609644003|\n",
      "|2007-02-28|         6.25|      11475|         0.3095|   88.4|       4.4|15767.146| 0.609644003|\n",
      "|2007-03-31|         6.25|    11490.1|         0.1316|   87.1|       4.5|15767.146| 0.609644003|\n",
      "|2007-04-30|         6.25|    11495.5|          0.047|   88.3|       4.4|15767.146| 0.609644003|\n",
      "|2007-05-31|         6.25|    11488.4|        -0.0618|   85.3|       4.6|15767.146| 0.609644003|\n",
      "|2007-06-30|         6.25|    11500.4|         0.1045|   90.4|       4.7|15767.146| 0.609644003|\n",
      "|2007-07-31|         5.75|    11506.2|         0.0504|   83.4|       4.6|15767.146| 0.609644003|\n",
      "|2007-08-31|         5.25|    11527.1|         0.1816|   83.4|       4.7|15767.146| 0.609644003|\n",
      "|2007-09-30|            5|    11523.6|        -0.0304|   80.9|       4.7|15767.146| 0.609644003|\n",
      "|2007-10-31|            5|    11512.3|        -0.0981|   76.1|       4.7|15767.146| 0.609644003|\n",
      "|2007-11-30|         4.75|    11552.1|         0.3457|   75.5|         5|15767.146| 0.609644003|\n",
      "|2007-12-31|          3.5|    11553.4|         0.0113|   78.4|         5|15767.146| 0.609644003|\n",
      "|2008-01-31|          3.5|    11547.6|        -0.0502|   70.8|       4.9|15366.607|-2.183097148|\n",
      "|2008-02-29|          2.5|    11545.3|        -0.0199|   69.5|       5.1|15366.607|-2.183097148|\n",
      "|2008-03-31|         2.25|    11500.7|        -0.3863|   62.6|         5|15366.607|-2.183097148|\n",
      "|2008-04-30|         2.25|    12060.9|          4.871|   59.8|       5.4|15366.607|-2.183097148|\n",
      "|2008-05-31|         2.25|    11755.3|        -2.5338|   56.4|       5.6|15366.607|-2.183097148|\n",
      "|2008-06-30|         2.25|      11574|        -1.5423|   61.2|       5.8|15366.607|-2.183097148|\n",
      "|2008-07-31|         2.25|    11506.9|        -0.5797|     63|       6.1|15366.607|-2.183097148|\n",
      "|2008-08-31|         2.25|    11531.5|         0.2138|   70.3|       6.1|15366.607|-2.183097148|\n",
      "+----------+-------------+-----------+---------------+-------+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Macro Economics\n",
    "\n",
    "\n",
    "gdp = spark.read.option(\"header\",True).csv(\"data/GDP_data_quarterly.csv\")\n",
    "\n",
    "# resample the GDP in pandas\n",
    "gdp = gdp.toPandas()\n",
    "gdp[\"date\"] = pd.to_datetime(gdp[\"date\"])\n",
    "gdp = gdp.set_index('date').resample('M').ffill()\n",
    "gdp = gdp.loc['2007-01-31':'2020-05-31']\n",
    "gdp = gdp.reset_index()\n",
    "gdp_rs = spark.createDataFrame(gdp) \n",
    "\n",
    "\n",
    "income = spark.read.option(\"header\",True).csv(\"data/income_data.csv\")\n",
    "interest = spark.read.option(\"header\",True).csv(\"data/interest_rate_data.csv\")\n",
    "umcs = spark.read.option(\"header\",True).csv(\"data/cust_senti_data.csv\")\n",
    "unemployment = spark.read.option(\"header\",True).csv(\"data/unemployment_data.csv\")\n",
    "\n",
    "macro_df = interest.join(income, \"date\",\"inner\")\n",
    "macro_df = macro_df.join(umcs,\"date\",\"inner\")\n",
    "macro_df = macro_df.join(unemployment,\"date\",\"inner\")\n",
    "macro_df = macro_df.withColumn(\"date\",to_date(col(\"date\"),\"dd/MM/yyyy\")) \n",
    "\n",
    "macro_df = macro_df.join(gdp_rs,\"date\",\"inner\")\n",
    "\n",
    "macro_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine macro with transction data\n",
    "df = df.join(macro_df,\"date\", \"inner\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py:114: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save data\n",
    "pd_df = df.toPandas()\n",
    "pd_df.to_csv(\"data/churn.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
